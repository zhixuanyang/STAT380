#use the full model
pred <- predict(gl_model, s = bestlam, newx = test)
rm(list = ls())
library(data.table)
library(caret)
library(Metrics)
library(glmnet)
library(plotmo)
library(lubridate)
interim_data_path = "~/Desktop/STAT380/themagic/project/volume/data/interim/"
model_path ="~/Desktop/STAT380/themagic/project/volume/models/"
submission_path = "~/Desktop/STAT380/themagicproject/volume/data/processed/"
raw_path = "~/Desktop/STAT380/themagic/project/volume/data/raw/"
train = fread(file.path(interim_data_path,"train_v1.csv"))
test = fread(file.path(interim_data_path,"test_v1.csv"))
example_sub = fread(file.path(raw_path,"example_submission.csv"))
View(train)
View(test)
rm(list = ls())
library(data.table)
library(caret)
library(Metrics)
library(glmnet)
library(plotmo)
library(lubridate)
interim_data_path = "~/Desktop/STAT380/themagic/project/volume/data/interim/"
model_path ="~/Desktop/STAT380/themagic/project/volume/models/"
submission_path = "~/Desktop/STAT380/themagicproject/volume/data/processed/"
raw_path = "~/Desktop/STAT380/themagic/project/volume/data/raw/"
train = fread(file.path(interim_data_path,"train_v1.csv"))
test = fread(file.path(interim_data_path,"test_v1.csv"))
example_sub = fread(file.path(raw_path,"example_submission.csv"))
test$future_price <- 0
year_num <- as.numeric(as.factor(year(as_date(train$current_date))))
# subset out only the columns to model
train
test
drops <-  c('id','future_date','current_date')
train <- train[, !drops, with = FALSE]
train
test <- test[, !drops, with = FALSE]
test
#save the response var because dummyVars will remove
train_y <- train$future_price
train_y
dummies  <-  dummyVars(future_price ~ ., data = train)
train <- predict(dummies, newdata = train)
test <- predict(dummies, newdata = test)
train <- data.table(train)
test <- data.table(test)
train <- as.matrix(train)
train
?cv.glmnet
?glmnet
gl_model <- cv.glmnet(train, train_y, alpha = 1, family = "gaussian", foldid = year_num, nfolds = length(unique(year_num)))
plot(gl_model)
bestlam <- gl_model$lambda.min
log(bestlam)
#fit a logistic model
gl_model <- glmnet(train, train_y, alpha = 1,family="gaussian")
plot(gl_model)
#save model
saveRDS(gl_model,".gl_model.model")
test <- as.matrix(test)
#use the full model
pred <- predict(gl_model, s = bestlam, newx = test)
example_sub$future_price <- pred
example_sub
#now we can write out a submission
rm(list = ls())
library(data.table)
library(caret)
library(Metrics)
library(glmnet)
library(plotmo)
library(lubridate)
interim_data_path = "~/Desktop/STAT380/themagic/project/volume/data/interim/"
model_path ="~/Desktop/STAT380/themagic/project/volume/models/"
submission_path = "~/Desktop/STAT380/themagicproject/volume/data/processed/"
raw_path = "~/Desktop/STAT380/themagic/project/volume/data/raw/"
train = fread(file.path(interim_data_path,"train_v1.csv"))
test = fread(file.path(interim_data_path,"test_v1.csv"))
example_sub = fread(file.path(raw_path,"example_submission.csv"))
test$future_price <- 0
year_num <- as.numeric(as.factor(year(as_date(train$current_date))))
# subset out only the columns to model
train
test
drops <-  c('id','future_date','current_date')
train <- train[, !drops, with = FALSE]
train
test <- test[, !drops, with = FALSE]
test
#save the response var because dummyVars will remove
train_y <- train$future_price
train_y
# work with dummies
dummies  <-  dummyVars(future_price ~ ., data = train)
train <- predict(dummies, newdata = train)
test <- predict(dummies, newdata = test)
train <- data.table(train)
test <- data.table(test)
########################
# Use cross validation #
########################
train <- as.matrix(train)
train
?cv.glmnet
?glmnet
gl_model <- cv.glmnet(train, train_y, alpha = 1, family = "gaussian", foldid = year_num, nfolds = length(unique(year_num)))
plot(gl_model)
rm(list = ls())
library(data.table)
library(caret)
library(Metrics)
library(glmnet)
library(plotmo)
library(lubridate)
interim_data_path = "~/Desktop/STAT380/themagic/project/volume/data/interim/"
model_path ="~/Desktop/STAT380/themagic/project/volume/models/"
submission_path = "~/Desktop/STAT380/themagicproject/volume/data/processed/"
raw_path = "~/Desktop/STAT380/themagic/project/volume/data/raw/"
train = fread(file.path(interim_data_path,"train_v1.csv"))
test = fread(file.path(interim_data_path,"test_v1.csv"))
example_sub = fread(file.path(raw_path,"example_submission.csv"))
test$future_price <- 0
year_num <- as.numeric(as.factor(year(as_date(train$current_date))))
year_num
View(train)
View(train)
rm(list = ls())
library(data.table)
library(caret)
library(Metrics)
library(glmnet)
library(plotmo)
library(lubridate)
interim_data_path = "~/Desktop/STAT380/themagic/project/volume/data/interim/"
model_path ="~/Desktop/STAT380/themagic/project/volume/models/"
submission_path = "~/Desktop/STAT380/themagicproject/volume/data/processed/"
raw_path = "~/Desktop/STAT380/themagic/project/volume/data/raw/"
train = fread(file.path(interim_data_path,"train_v1.csv"))
test = fread(file.path(interim_data_path,"test_v1.csv"))
example_sub = fread(file.path(raw_path,"example_submission.csv"))
test$future_price <- 0
year_num <- as.numeric(as.factor(year(as_date(train$current_date))))
# subset out only the columns to model
train
test
drops <-  c('id','future_date','current_date')
train <- train[, !drops, with = FALSE]
train
test <- test[, !drops, with = FALSE]
test
#save the response var because dummyVars will remove
train_y <- train$future_price
train_y
# work with dummies
dummies  <-  dummyVars(future_price ~ ., data = train)
train <- predict(dummies, newdata = train)
test <- predict(dummies, newdata = test)
train <- data.table(train)
test <- data.table(test)
########################
# Use cross validation #
########################
train <- as.matrix(train)
train
?cv.glmnet
?glmnet
gl_model <- cv.glmnet(train, train_y, alpha = 1, family = "gaussian", foldid = year_num, nfolds = length(unique(year_num)))
plot(gl_model)
rm(list = ls())
library(data.table)
library(Rtsne)
library(ggplot2)
library(caret)
set.seed(27)
Ap_Pres<-c(0,1)
UBC<-c(0,1)
R2C<-c(0,1)
GSHC<-c(0,1)
Fox<-c(0,1)
CNN<-c(0,1)
Lower_Immigration<-c(0,1)
Glob_W<-c(0,1)
Tax_high_income<-c(0,1)
Death_Pen<-c(0,1)
Fed_too_much_Power<-c(0,1)
Stricter_Gun_Laws<-c(0,1)
Stricter_Env_laws<-c(0,1)
R_Q1<-c(0.11,0.89) #https://news.gallup.com/poll/203198/presidential-approval-ratings-donald-trump.aspx
R_Q2<-c(.15,0.85) #https://www.washingtonpost.com/politics/americans-of-both-parties-overwhelmingly-support-red-flag-laws-expanded-gun-background-checks-washington-post-abc-news-poll-finds/2019/09/08/97208916-ca75-11e9-a4f3-c081a126de70_story.html
R_Q3<-c(0.65,0.35) #https://fivethirtyeight.com/features/the-abortion-debate-isnt-as-partisan-as-politicians-make-it-seem/
R_Q4<-c(0.88,0.12) #https://www.pewresearch.org/fact-tank/2018/10/03/most-continue-to-say-ensuring-health-care-coverage-is-governments-responsibility/
R_Q5<-c(0.3,0.7) #https://www.businessinsider.com/most-and-least-trusted-news-outlets-in-america-cnn-fox-news-new-york-times-2019-4
R_Q6<-c(0.7,0.3) #https://www.businessinsider.com/most-and-least-trusted-news-outlets-in-america-cnn-fox-news-new-york-times-2019-4
R_Q7<-c(0.4,0.6) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
R_Q8<-c(0.6,0.4) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
R_Q9<-c(0.6,0.4) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
R_Q10<-c(0.2,0.8) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
R_Q11<-c(0.18,0.82) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
R_Q12<-c(0.66,0.34) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
R_Q13<-c(0.65,0.35) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q1<-c(0.93,0.07) #https://news.gallup.com/poll/203198/presidential-approval-ratings-donald-trump.aspx
D_Q2<-c(0.05,0.95) #https://www.washingtonpost.com/politics/americans-of-both-parties-overwhelmingly-support-red-flag-laws-expanded-gun-background-checks-washington-post-abc-news-poll-finds/2019/09/08/97208916-ca75-11e9-a4f3-c081a126de70_story.html
D_Q3<-c(0.25,0.75) #https://fivethirtyeight.com/features/the-abortion-debate-isnt-as-partisan-as-politicians-make-it-seem/
D_Q4<-c(0.49,0.51) #https://www.pewresearch.org/fact-tank/2018/10/03/most-continue-to-say-ensuring-health-care-coverage-is-governments-responsibility/
D_Q5<-c(0.58,0.42) #https://www.businessinsider.com/most-and-least-trusted-news-outlets-in-america-cnn-fox-news-new-york-times-2019-4
D_Q6<-c(0.18,0.82) #https://www.businessinsider.com/most-and-least-trusted-news-outlets-in-america-cnn-fox-news-new-york-times-2019-4
D_Q7<-c(0.8,0.2) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q8<-c(0.11,0.89) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q9<-c(0.18,0.82) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q10<-c(0.59,0.41) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q11<-c(0.64,0.36) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q12<-c(0.23,0.77) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
D_Q13<-c(0.21,0.79) #https://news.gallup.com/opinion/polling-matters/215210/partisan-differences-growing-number-issues.aspx
Qs<-c("Ap_Pres","UBC","R2C","GSHC","Fox","CNN","Lower_Immigration","Glob_W","Tax_high_income","Death_Pen","Fed_too_much_Power","Stricter_Gun_Laws","Stricter_Env_laws")
party<-c("R","D")
q_data<-NULL
for (j in 1:length(party)){
sp_tab<-NULL
for (i in 1:length(Qs)){
msats<-sample(get(Qs[i]),5000,replace=T,prob=get(paste0(party[j],"_Q",i)))
sp_tab<-cbind(sp_tab,msats)
sp_tab<-data.table(sp_tab)
names(sp_tab)[i]<-Qs[i]
}
sp_tab$party<-party[j]
q_data<-rbind(q_data,sp_tab)
}
q_data$Cats<-sample(c(0,1),dim(q_data)[1],replace=T)
q_data$Knitting<-0
q_data[Cats==1]$Knitting<-sample(c(0,1),dim(q_data[Cats==1])[1],replace=T,prob=c(0.15,0.85))
q_data[Cats==0]$Knitting<-sample(c(0,1),dim(q_data[Cats==0])[1],replace=T,prob=c(0.85,0.15))
q_data<-q_data[,.(party,Ap_Pres,UBC,R2C,GSHC,Fox,CNN,Lower_Immigration,Glob_W,Tax_high_income,Death_Pen,Fed_too_much_Power,Stricter_Gun_Laws,Stricter_Env_laws,Cats,Knitting)]
fwrite(q_data,"~/Desktop/STAT380/PCA/project/volume/data/raw/data.csv")
fwrite(q_data,"~/Desktop/STAT380/PCA Test/project/volume/data/raw/data.csv")
rm(list = ls())
library(data.table)
library(Rtsne)
library(ggplot2)
library(caret)
library(ggplot2)
library(ClusterR)
set.seed(3)
# load in data
data<-fread("~/Desktop/STAT380/PCA Test/project/volume/data/raw/data.csv")
# we are not supposed to know the party of the individuals so we should hide this
party<-data$party
data$party<-NULL
j_data<-data.frame(lapply(data, jitter,factor=0.01))
# do a pca
pca<-prcomp(j_data)
# look at the percent variance explained by each pca
screeplot(pca)
# look at the rotation of the variables on the PCs
pca
# see the values of the scree plot in a table
summary(pca)
# use the unclass() function to get the data in PCA space
pca_dt<-data.table(unclass(pca)$x)
# add back the party to prove to ourselves that this works
pca_dt$party<-party
# see a plot with the party data
ggplot(pca_dt,aes(x=PC1,y=PC2,col=party))+geom_point()
tsne<-Rtsne(pca_dt,pca = F)
tsne
help(Rtsne)
View(tsne)
rm(list = ls())
library(data.table)
library(Rtsne)
library(ggplot2)
library(caret)
library(ggplot2)
library(ClusterR)
set.seed(3)
# load in data
data<-fread("~/Desktop/STAT380/PCA Test/project/volume/data/raw/data.csv")
# we are not supposed to know the party of the individuals so we should hide this
party<-data$party
data$party<-NULL
j_data<-data.frame(lapply(data, jitter,factor=0.01))
# do a pca
pca<-prcomp(j_data)
# look at the percent variance explained by each pca
screeplot(pca)
# look at the rotation of the variables on the PCs
pca
# see the values of the scree plot in a table
summary(pca)
# see a biplot of the first 2 PCs
biplot(pca)
# use the unclass() function to get the data in PCA space
pca_dt<-data.table(unclass(pca)$x)
rm(list = ls())
library(data.table)
library(Rtsne)
library(ggplot2)
library(caret)
library(ggplot2)
library(ClusterR)
set.seed(3)
# load in data
data<-fread("~/Desktop/STAT380/PCA Test/project/volume/data/raw/data.csv")
# we are not supposed to know the party of the individuals so we should hide this
party<-data$party
data$party<-NULL
j_data<-data.frame(lapply(data, jitter,factor=0.01))
# do a pca
pca<-prcomp(j_data)
# look at the percent variance explained by each pca
screeplot(pca)
# look at the rotation of the variables on the PCs
pca
# see the values of the scree plot in a table
summary(pca)
# use the unclass() function to get the data in PCA space
pca_dt<-data.table(unclass(pca)$x)
View(pca_dt)
unclass(pca)
rm(list = ls())
library(data.table)
library(Rtsne)
library(ggplot2)
library(caret)
library(ggplot2)
library(ClusterR)
set.seed(3)
# load in data
data<-fread("~/Desktop/STAT380/PCA Test/project/volume/data/raw/data.csv")
# we are not supposed to know the party of the individuals so we should hide this
party <- data$party
data$party <- NULL
j_data <- data.frame(lapply(data, jitter, factor = 0.01))
# do a pca
pca <- prcomp(j_data)
# use the unclass() function to get the data in PCA space
pca_dt <- data.table(unclass(pca)$x)
pca_dt$party = NULL  # We DO NOT know the party, so we should
# remove it when doing the PCA
tsne_pca <- Rtsne(pca_dt, pca = F)  # do this, if you have already done PCA
tsne_data <- Rtsne(j_data, pca = T) # do this, if you have not run PCA
# need to use j_data, can't use data
# grab the x-y coodinrates for each observation
tsne_dt_pca <- data.table(tsne_pca$Y)
tsne_dt_data <- data.table(tsne_data$Y)
# Let's color by party to see how this separted parties
ggplot(tsne_dt_pca, aes(x = V1, y = V2)) + geom_point()
ggplot(tsne_dt_pca, aes(x = V1, y = V2, col = party)) + geom_point()
ggplot(tsne_dt_data, aes(x = V1, y = V2)) + geom_point()
ggplot(tsne_dt_data, aes(x = V1, y = V2, col = party)) + geom_point()
gmm_tsne_aic <- Optimal_Clusters_GMM(tsne_dt_pca,
max_clusters = 10,
criterion = "AIC")
# I am going to run this second on the first two principle components
# as found in pca_dt
gmm_pca2_aic <- Optimal_Clusters_GMM(pca_dt[, .(PC1, PC2)],
max_clusters = 10,
criterion = "AIC")
# I am going to run this third on the first three principle components
# as found in pca_dt (since the third one *might* be important)
gmm_pca3_aic <- Optimal_Clusters_GMM(pca_dt[, .(PC1, PC2, PC3)],
max_clusters = 10,
criterion = "AIC")
# I am going to run this last on the orginal data, since it has not
# been tampered with.  We are using full dimesions, but since there
# are only 15 columns, this isn't a problem (as opposed to having hundreds)
gmm_data_aic <- Optimal_Clusters_GMM(j_data,
max_clusters = 10,
criterion = "AIC")
# now we will look at the change in model fit between successive k values
delta_data_k <- gmm_data_aic[-1] - gmm_data_aic[-10]
delta_pca2_k <- gmm_pca2_aic[-1] - gmm_pca2_aic[-10]
delta_pca3_k <- gmm_pca3_aic[-1] - gmm_pca3_aic[-10]
delta_tsne_k <- gmm_tsne_aic[-1] - gmm_tsne_aic[-10]
# I'm going to make a plot so you can see the values, this part isnt necessary
del_k_tab <- data.table(delta_tsne_k,
delta_pca2_k,
delta_pca3_k,
delta_data_k, k = 2:10)
# plot.  We are looking for the optimal number of clusters
ggplot(del_k_tab, aes(x = k, y = -delta_tsne_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
ggplot(del_k_tab, aes(x = k, y = -delta_pca2_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
ggplot(del_k_tab, aes(x = k, y = -delta_pca3_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
ggplot(del_k_tab, aes(x = k, y = -delta_data_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
# now we run the model with our chosen k value
# gmm_pca <- GMM(pca_dt[, .(PC1, PC2)], opt_k)
gmm <- GMM(j_data, opt_k)
opt_k <- 2
# now we run the model with our chosen k value
# gmm_pca <- GMM(pca_dt[, .(PC1, PC2)], opt_k)
gmm <- GMM(j_data, opt_k)
# plot.  We are looking for the optimal number of clusters
ggplot(del_k_tab, aes(x = k, y = -delta_tsne_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
ggplot(del_k_tab, aes(x = k, y = -delta_pca2_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
ggplot(del_k_tab, aes(x = k, y = -delta_pca3_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
ggplot(del_k_tab, aes(x = k, y = -delta_data_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
opt_k <- 2
# now we run the model with our chosen k value
# gmm_pca <- GMM(pca_dt[, .(PC1, PC2)], opt_k)
gmm <- GMM(j_data, opt_k)
cluster_prob_data = predict_GMM(j_data,
gmm$centroids,
gmm$covariance_matrices,
gmm$weights)$cluster_proba
cluster_prob_data = as.data.table(cluster_prob_data)
cluster_prob_data
ggplot(tsne_dt_data,
aes(x = V1, y = V2, col = cluster_prob_data$V1)) +
geom_point()
# I'm going to check out how well I did.
tab = table(round(cluster_prob_data$V1), party)
tab
(tab[1, 1] + tab[2, 2]) / 10000
# Let's see how well we would have done if we used tsne data with 2 clusters
gmm <- GMM(tsne_dt_pca, 2)
# plot.  We are looking for the optimal number of clusters
ggplot(del_k_tab, aes(x = k, y = -delta_tsne_k)) + geom_point() + geom_line() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
geom_text(aes(label = k), hjust = 0, vjust = -1)
# Let's see how well we would have done if we used tsne data with 2 clusters
gmm <- GMM(tsne_dt_pca, 2)
cluster_prob_data = predict_GMM(tsne_dt_pca,
gmm$centroids,
gmm$covariance_matrices,
gmm$weights)$cluster_proba
cluster_prob_data = as.data.table(cluster_prob_data)
cluster_prob_data
ggplot(tsne_dt_data,
aes(x = V1, y = V2, col = cluster_prob_data$V1)) +
geom_point()
# I'm going to check out how well I did.
tab = table(round(cluster_prob_data$V1), party)
tab
# Let's see what would happen with 4 clusters when applied to tsne data
gmm <- GMM(tsne_dt_pca, 4)
cluster_prob_data = predict_GMM(tsne_dt_pca,
gmm$centroids,
gmm$covariance_matrices,
gmm$weights)$cluster_proba
cluster_prob_data = as.data.table(cluster_prob_data)
cluster_prob_data
cluster = rep("", 10000)
for (i in 1:10000) {  # I probably should use lapply or apply here, but I don't want to
cluster[i] = names(which.max(cluster_prob_data[i]))
}
table(cluster, party)
ggplot(tsne_dt_data,
aes(x = V1, y = V2, col = cluster)) +
geom_point()
ggplot(tsne_dt_data,
aes(x = V1, y = V2, col = party)) +
geom_point()
# Let's see what would happen with 3 cluster applied to pca_data, one just the
# first to components
gmm <- GMM(pca_dt[, .(PC1, PC2)], 3)
cluster_prob_data = predict_GMM(pca_dt[, .(PC1, PC2)],
gmm$centroids,
gmm$covariance_matrices,
gmm$weights)$cluster_proba
cluster_prob_data = as.data.table(cluster_prob_data)
cluster_prob_data
cluster = rep("", 10000)
for (i in 1:10000) { # ditto above
cluster[i] = names(which.max(cluster_prob_data[i]))
}
table(cluster, party)
cats = data$Cats
knitting = data$Knitting
table(cluster, party, cats, knitting)
ggplot(tsne_dt_data,
aes(x = V1, y = V2, col = cluster)) +
geom_point()
ggplot(tsne_dt_data,
aes(x = V1, y = V2, col = party)) +
geom_point()
# Let's go back to the correct solution given our knowledge that there
# are two polictical parties.
gmm <- GMM(j_data, 2)
cluster_prob_data = predict_GMM(j_data,
gmm$centroids,
gmm$covariance_matrices,
gmm$weights)$cluster_proba
cluster_prob_data = as.data.table(cluster_prob_data)
cluster_prob_data
solution = data.table(Id = 1:10000,
Prob1 = cluster_prob_data$V1,
Prob2 = cluster_prob_data$V2)
solution = data.table(Id = 1:10000,
Prob1 = cluster_prob_data$V2,
Prob2 = cluster_prob_data$V1)
solution
fwrite(solution, file = "mysolution.csv")
# then submit this.
